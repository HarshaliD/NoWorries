{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63cfee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 1: Imports\n",
    "# =========================\n",
    "import os\n",
    "import tempfile\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48996554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2: Paths\n",
    "# =========================\n",
    "DATA_DIR = \"./data/docs\"\n",
    "OUTPUT_DIR = \"./vectorstore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ebb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3: OCR fallback function\n",
    "# =========================\n",
    "def extract_text_ocr(pdf_path):\n",
    "    text = \"\"\n",
    "    with tempfile.TemporaryDirectory() as path:\n",
    "        images = convert_from_path(pdf_path, dpi=200, output_folder=path)\n",
    "        for i, img in enumerate(images):\n",
    "            text += f\"\\n--- Page {i+1} ---\\n\" + pytesseract.image_to_string(img)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6afd55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4: Load PDFs\n",
    "# =========================\n",
    "def load_pdfs():\n",
    "    docs = []\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        if not file.endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(DATA_DIR, file)\n",
    "        try:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            text = \" \".join([p.page_content for p in pages])\n",
    "            if len(text.strip()) < 50:  # too little text, use OCR\n",
    "                text = extract_text_ocr(pdf_path)\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": file}))\n",
    "        except Exception:\n",
    "            text = extract_text_ocr(pdf_path)\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": file}))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404b53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5: Build vectorstore\n",
    "# =========================\n",
    "def build_vectorstore():\n",
    "    docs = load_pdfs()\n",
    "    print(f\"‚úÖ Loaded {len(docs)} PDFs\")\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Total chunks created: {len(chunks)}\")\n",
    "    \n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedder)\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    vectorstore.save_local(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Vectorstore saved at {OUTPUT_DIR}\")\n",
    "    \n",
    "    return vectorstore, chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aea9cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 PDFs\n",
      "‚úÖ Total chunks created: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_12472\\2439433745.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vectorstore saved at ./vectorstore\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6: Run build\n",
    "# =========================\n",
    "vectorstore, chunks = build_vectorstore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a8458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks and their source PDFs:\n",
      "\n",
      "Chunk 1 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "Panic\n",
      "A self-help guide\n",
      "Workbook 13\n",
      "Revised  January 2024 v1 2\n",
      "ÔÅ± Introduction‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶3\n",
      "ÔÅ± Panic attacks‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.4\n",
      "ÔÅ± Stress and hyperventilation‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.....6\n",
      "ÔÅ± Panic attack v \n",
      "---\n",
      "\n",
      "Chunk 2 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "ÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19\n",
      "ÔÅ± Graded exposure‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.21\n",
      "ÔÅ± My graded hierarchy‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...24\n",
      "ÔÅ± My exposure diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..........25\n",
      "ÔÅ± Tips for dealing with panic‚Ä¶‚Ä¶‚Ä¶ \n",
      "---\n",
      "\n",
      "Chunk 3 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "Contents of \n",
      "       this booklet 3\n",
      "Most people will feel anxious at some point in their \n",
      "lives, but anxiety becomes a problem when it starts to \n",
      "negatively impact your day-to-day life.\n",
      "This workbook w \n",
      "---\n",
      "\n",
      "Chunk 4 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "threatening and we don‚Äôt feel we have the skills we need to cope \n",
      "with it.\n",
      "If something causes us to feel afraid or scared, then this can induce \n",
      "physical symptoms known as the ‚Äòfight or flight‚Äô respo \n",
      "---\n",
      "\n",
      "Chunk 5 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "prepare to fight it.\n",
      "The ‚Äòfight or flight‚Äô response is helpful if we are faced with real \n",
      "danger but for most of us our daily lives do not present us with life-\n",
      "threatening situations.\n",
      "When we are fac \n",
      "---\n",
      "\n",
      "Chunk 6 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "with the situation, and can actually be quite frightening.\n",
      "Introduction\n",
      "Did you know? \n",
      "1 in 6 people in the UK \n",
      "experience a common \n",
      "mental health difficulty \n",
      "like anxiety in any given \n",
      "week according \n",
      "---\n",
      "\n",
      "Chunk 7 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "experience a panic attack.\n",
      "During a panic attack we may experience intense fear, terror and catastrophic \n",
      "thoughts that occur suddenly and often peak within 5-10 minutes.\n",
      "Panic attacks are not dangero \n",
      "---\n",
      "\n",
      "Chunk 8 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "may find the strategies in this workbook helpful.\n",
      "What are\n",
      " panic attacks?\n",
      "Reflect\n",
      "When you panic, can you relate to feeling any of the following:\n",
      "‚Ä¢ ‚ÄúI feel like I‚Äôm going crazy, like I‚Äôm going to los \n",
      "---\n",
      "\n",
      "Chunk 9 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "‚Ä¢ Are you worried about these feelings coming back?\n",
      "‚Ä¢ Do you find these feelings stop you from doing things you want to do? 5\n",
      "How to recognise a panic attack\n",
      "A ‚Äòhot cross bun‚Äô for a panic attack could \n",
      "---\n",
      "\n",
      "Chunk 10 | Source: gsh-13_v1_panic_january24_final.pdf\n",
      "situations\n",
      "Feelings\n",
      "Anxious, scared, \n",
      "ashamed, frustrated, \n",
      "upset\n",
      "Reflect\n",
      "What thoughts cross your mind when you‚Äôre experiencing a panic attack?\n",
      "How do you feel emotionally?\n",
      "What physical symptoms do  \n",
      "---\n",
      "\n",
      "Total chunks loaded: 181\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 7: Interactive chunk inspection\n",
    "# =========================\n",
    "print(\"Sample chunks and their source PDFs:\\n\")\n",
    "for i, chunk in enumerate(chunks[:10]):  # first 10 chunks\n",
    "    print(f\"Chunk {i+1} | Source: {chunk.metadata['source']}\")\n",
    "    print(chunk.page_content[:200], \"\\n---\\n\")  # first 200 characters\n",
    "\n",
    "print(f\"Total chunks loaded: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95396cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 results for query: 'Example question about PDFs'\n",
      "\n",
      "Source: Panic-ER-final-2022.pdf\n",
      "Review date 2025 \n",
      "ISBN:978-1-909664-34-0\n",
      " \n",
      "Follow us on Twitter @cntwnhs and Facebook CNTWNHS \n",
      "---\n",
      "\n",
      "Source: Panic-ER-final-2022.pdf\n",
      "this leaflet please get in touch.  \n",
      "This information can be made available in a range of formats on \n",
      "request (eg Braille, audio, larger print, BSL or other languages). \n",
      "Please contact the Patient Information Centre Tel: 0191 246 7288 \n",
      "Published by the Patient Information Centre \n",
      "2022 Copyright, Cumb \n",
      "---\n",
      "\n",
      "Source: gsh-13_v1_panic_january24_final.pdf\n",
      "‚Äì either by leaving or using a safety behaviour. \n",
      "Each time we face that same situation our anxiety is equally \n",
      "high the next time round:\n",
      "Facing your fears 20\n",
      "What would actually happen?\n",
      "This graph shows what would actually happen if \n",
      "we were to face an anxiety provoking situation \n",
      "without the suppo \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 8: Similarity search example\n",
    "# =========================\n",
    "query = \"Example question about PDFs\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Top 3 results for query: '{query}'\\n\")\n",
    "for r in results:\n",
    "    print(f\"Source: {r.metadata['source']}\")\n",
    "    print(r.page_content[:300], \"\\n---\\n\")  # first 300 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a9d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 PDFs\n",
      "‚úÖ Total chunks created: 181\n",
      "‚úÖ Vectorstore saved at ./vectorstore\n",
      "\n",
      "=== Sample chunks (first 5) ===\n",
      "                         source_pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      chunk_text  chunk_length\n",
      "gsh-13_v1_panic_january24_final.pdf Panic\\nA self-help guide\\nWorkbook 13\\nRevised  January 2024 v1 2\\nÔÅ± Introduction‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶3\\nÔÅ± Panic attacks‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.4\\nÔÅ± Stress and hyperventilation‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.....6\\nÔÅ± Panic attack vs heart attack‚Ä¶............................................8\\nÔÅ± What keeps panic attacks going?‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.‚Ä¶‚Ä¶‚Ä¶‚Ä¶10\\nÔÅ± What might keep my panic attacks going‚Ä¶‚Ä¶‚Ä¶‚Ä¶......‚Ä¶.14\\nÔÅ± The vicious cycle of panic‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.........15\\nÔÅ± My panic diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.......18\\nÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19           499\n",
      "gsh-13_v1_panic_january24_final.pdf                                                       ÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19\\nÔÅ± Graded exposure‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.21\\nÔÅ± My graded hierarchy‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...24\\nÔÅ± My exposure diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..........25\\nÔÅ± Tips for dealing with panic‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.26\\nÔÅ± Further resources‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....27                                                          \\nÔÅ± Useful contacts‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....28                                                            \\nContents of \\n       this booklet 3           449\n",
      "gsh-13_v1_panic_january24_final.pdf    Contents of \\n       this booklet 3\\nMost people will feel anxious at some point in their \\nlives, but anxiety becomes a problem when it starts to \\nnegatively impact your day-to-day life.\\nThis workbook will introduce tools in line with Cognitive Behavioural \\nTherapy, a treatment that is recommended by NICE (National \\nInstitute of Care Excellence) and backed by evidence as one of the \\nmost effective ways to improve anxiety. \\nWhat is anxiety?\\nAnxiety occurs when we perceive a situation or object as           498\n",
      "gsh-13_v1_panic_january24_final.pdf                           threatening and we don‚Äôt feel we have the skills we need to cope \\nwith it.\\nIf something causes us to feel afraid or scared, then this can induce \\nphysical symptoms known as the ‚Äòfight or flight‚Äô response. \\nThis a response that has evolved to protect us from danger and \\nincludes symptoms such as increased heart rate, feeling hot and \\nfeeling a surge of adrenaline, among others. These physical \\nsymptoms occur to help us either run away from danger or to \\nprepare to fight it.           477\n",
      "gsh-13_v1_panic_january24_final.pdf                                           prepare to fight it.\\nThe ‚Äòfight or flight‚Äô response is helpful if we are faced with real \\ndanger but for most of us our daily lives do not present us with life-\\nthreatening situations.\\nWhen we are faced with any situation that we don‚Äôt feel prepared to \\ndeal with, the ‚Äòfight or flight‚Äô response can become activated. If \\nthe situation is not physically threatening (e.g. giving a presentation \\nto a large group of people) this response is not helpful for coping           462\n",
      "\n",
      "=== Chunk stats per PDF ===\n",
      "                                                    total_chunks  \\\n",
      "source_pdf                                                         \n",
      "panic-a-self-help-guide.pdf                                   77   \n",
      "gsh-13_v1_panic_january24_final.pdf                           53   \n",
      "Panic Attacks & Panic Disorder_ Causes, Symptom...            30   \n",
      "Panic-ER-final-2022.pdf                                       15   \n",
      "How-can-I-help-someone-having-a-panic-attack-Ac...             6   \n",
      "\n",
      "                                                    avg_chunk_length  \n",
      "source_pdf                                                            \n",
      "panic-a-self-help-guide.pdf                               460.974026  \n",
      "gsh-13_v1_panic_january24_final.pdf                       467.603774  \n",
      "Panic Attacks & Panic Disorder_ Causes, Symptom...        454.933333  \n",
      "Panic-ER-final-2022.pdf                                   449.866667  \n",
      "How-can-I-help-someone-having-a-panic-attack-Ac...        457.500000  \n",
      "\n",
      "=== Top 3 results for query: 'Example question about PDFs' ===\n",
      "\n",
      "Source: Panic-ER-final-2022.pdf\n",
      "Review date 2025 \n",
      "ISBN:978-1-909664-34-0\n",
      " \n",
      "Follow us on Twitter @cntwnhs and Facebook CNTWNHS \n",
      "---\n",
      "\n",
      "\n",
      "Source: Panic-ER-final-2022.pdf\n",
      "this leaflet please get in touch.  \n",
      "This information can be made available in a range of formats on \n",
      "request (eg Braille, audio, larger print, BSL or other languages). \n",
      "Please contact the Patient Information Centre Tel: 0191 246 7288 \n",
      "Published by the Patient Information Centre \n",
      "2022 Copyright, Cumb \n",
      "---\n",
      "\n",
      "\n",
      "Source: gsh-13_v1_panic_january24_final.pdf\n",
      "‚Äì either by leaving or using a safety behaviour. \n",
      "Each time we face that same situation our anxiety is equally \n",
      "high the next time round:\n",
      "Facing your fears 20\n",
      "What would actually happen?\n",
      "This graph shows what would actually happen if \n",
      "we were to face an anxiety provoking situation \n",
      "without the suppo \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Complete RAG Preprocessing Script\n",
    "# =========================\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_DIR = \"./data/docs\"\n",
    "OUTPUT_DIR = \"./vectorstore\"\n",
    "\n",
    "# -------------------------\n",
    "# OCR fallback function\n",
    "# -------------------------\n",
    "def extract_text_ocr(pdf_path):\n",
    "    text = \"\"\n",
    "    with tempfile.TemporaryDirectory() as path:\n",
    "        images = convert_from_path(pdf_path, dpi=200, output_folder=path)\n",
    "        for i, img in enumerate(images):\n",
    "            text += f\"\\n--- Page {i+1} ---\\n\" + pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "# -------------------------\n",
    "# Load PDFs\n",
    "# -------------------------\n",
    "def load_pdfs():\n",
    "    docs = []\n",
    "    for file in os.listdir(DATA_DIR):\n",
    "        if not file.endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_path = os.path.join(DATA_DIR, file)\n",
    "        try:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            text = \" \".join([p.page_content for p in pages])\n",
    "            if len(text.strip()) < 50:  # too little text, use OCR\n",
    "                text = extract_text_ocr(pdf_path)\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": file}))\n",
    "        except Exception:\n",
    "            text = extract_text_ocr(pdf_path)\n",
    "            docs.append(Document(page_content=text, metadata={\"source\": file}))\n",
    "    return docs\n",
    "\n",
    "# -------------------------\n",
    "# Build Vectorstore\n",
    "# -------------------------\n",
    "def build_vectorstore():\n",
    "    docs = load_pdfs()\n",
    "    print(f\"‚úÖ Loaded {len(docs)} PDFs\")\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Total chunks created: {len(chunks)}\")\n",
    "    \n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedder)\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    vectorstore.save_local(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Vectorstore saved at {OUTPUT_DIR}\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Inspect Chunks\n",
    "    # -------------------------\n",
    "    df_chunks = pd.DataFrame({\n",
    "        \"source_pdf\": [c.metadata[\"source\"] for c in chunks],\n",
    "        \"chunk_text\": [c.page_content for c in chunks],\n",
    "        \"chunk_length\": [len(c.page_content) for c in chunks]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Sample chunks (first 5) ===\")\n",
    "    print(df_chunks.head(5).to_string(index=False))\n",
    "    \n",
    "    stats = df_chunks.groupby(\"source_pdf\").agg(\n",
    "        total_chunks=(\"chunk_text\", \"count\"),\n",
    "        avg_chunk_length=(\"chunk_length\", \"mean\")\n",
    "    ).sort_values(by=\"total_chunks\", ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Chunk stats per PDF ===\")\n",
    "    print(stats)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Similarity search example\n",
    "    # -------------------------\n",
    "    query = \"Example question about PDFs\"\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    print(f\"\\n=== Top 3 results for query: '{query}' ===\")\n",
    "    for r in results:\n",
    "        print(f\"\\nSource: {r.metadata['source']}\")\n",
    "        print(r.page_content[:300], \"\\n---\\n\")\n",
    "    \n",
    "    return vectorstore, chunks, df_chunks\n",
    "\n",
    "# -------------------------\n",
    "# Run\n",
    "# -------------------------\n",
    "vectorstore, chunks, df_chunks = build_vectorstore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe76696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Loading PDFs\n",
      "============================================================\n",
      "üìÇ Found 5 PDF files\n",
      "\n",
      "‚úÖ gsh-13_v1_panic_january24_final.pdf: Standard extraction OK (23841 chars)\n",
      "‚úÖ How-can-I-help-someone-having-a-panic-attack-Accessible.pdf: Standard extraction OK (2751 chars)\n",
      "‚úÖ Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf: Standard extraction OK (13366 chars)\n",
      "‚úÖ panic-a-self-help-guide.pdf: Standard extraction OK (34936 chars)\n",
      "‚úÖ Panic-ER-final-2022.pdf: Standard extraction OK (6360 chars)\n",
      "\n",
      "‚úÖ Successfully loaded 5 PDFs\n",
      "\n",
      "============================================================\n",
      "Processing Summary\n",
      "============================================================\n",
      "                                                            file      method  text_length\n",
      "                             gsh-13_v1_panic_january24_final.pdf PyPDFLoader        23842\n",
      "     How-can-I-help-someone-having-a-panic-attack-Accessible.pdf PyPDFLoader         2751\n",
      "Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf PyPDFLoader        13366\n",
      "                                     panic-a-self-help-guide.pdf PyPDFLoader        34936\n",
      "                                         Panic-ER-final-2022.pdf PyPDFLoader         6360\n",
      "\n",
      "OCR Usage: 0 / 5 PDFs\n",
      "\n",
      "============================================================\n",
      "STEP 2: Chunking Documents\n",
      "============================================================\n",
      "‚úÖ Created 181 chunks\n",
      "\n",
      "============================================================\n",
      "STEP 3: Creating Vector Embeddings\n",
      "============================================================\n",
      "‚úÖ Vectorstore saved at ./vectorstore\n",
      "\n",
      "============================================================\n",
      "STEP 4: Chunk Analysis\n",
      "============================================================\n",
      "\n",
      "üìÑ Sample chunks (first 3):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Source: gsh-13_v1_panic_january24_final.pdf (PyPDFLoader)\n",
      "Text: Panic\n",
      "A self-help guide\n",
      "Workbook 13\n",
      "Revised  January 2024 v1 2\n",
      "ÔÅ± Introduction‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶3\n",
      "ÔÅ± Panic attacks‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.4\n",
      "ÔÅ± Stress and hyperventilation‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.....6\n",
      "ÔÅ± Panic attack v...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Source: gsh-13_v1_panic_january24_final.pdf (PyPDFLoader)\n",
      "Text: ÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19\n",
      "ÔÅ± Graded exposure‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.21\n",
      "ÔÅ± My graded hierarchy‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...24\n",
      "ÔÅ± My exposure diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..........25\n",
      "ÔÅ± Tips for dealing with panic‚Ä¶‚Ä¶‚Ä¶...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Source: gsh-13_v1_panic_january24_final.pdf (PyPDFLoader)\n",
      "Text: Contents of \n",
      "       this booklet 3\n",
      "Most people will feel anxious at some point in their \n",
      "lives, but anxiety becomes a problem when it starts to \n",
      "negatively impact your day-to-day life.\n",
      "This workbook w...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Chunk Statistics by PDF:\n",
      "                                                                  total_chunks  avg_chunk_length       method\n",
      "source_pdf                                                                                                   \n",
      "panic-a-self-help-guide.pdf                                                 77        460.974026  PyPDFLoader\n",
      "gsh-13_v1_panic_january24_final.pdf                                         53        467.603774  PyPDFLoader\n",
      "Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf            30        454.933333  PyPDFLoader\n",
      "Panic-ER-final-2022.pdf                                                     15        449.866667  PyPDFLoader\n",
      "How-can-I-help-someone-having-a-panic-attack-Accessible.pdf                  6        457.500000  PyPDFLoader\n",
      "\n",
      "============================================================\n",
      "STEP 5: Testing Retrieval\n",
      "============================================================\n",
      "\n",
      "üîé Query: 'What information is available in these documents?'\n",
      "üìå Top 3 results:\n",
      "\n",
      "1. Source: Panic-ER-final-2022.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: Review date 2025 \n",
      "ISBN:978-1-909664-34-0\n",
      " \n",
      "Follow us on Twitter @cntwnhs and Facebook CNTWNHS...\n",
      "\n",
      "2. Source: gsh-13_v1_panic_january24_final.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: ÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19\n",
      "ÔÅ± Graded exposure‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.21\n",
      "ÔÅ± My graded hierarchy‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...24\n",
      "ÔÅ± My exposure diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..........25\n",
      "ÔÅ± Tips for dealing with panic‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.26\n",
      "ÔÅ± Further resources‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....\n",
      "\n",
      "3. Source: Panic-ER-final-2022.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: this leaflet please get in touch.  \n",
      "This information can be made available in a range of formats on \n",
      "request (eg Braille, audio, larger print, BSL or other languages). \n",
      "Please contact the Patient Information Centre Tel: 0191 246 7288 \n",
      "Published by th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Smart RAG with Auto-OCR Detection\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "DATA_DIR = \"./data/docs\"\n",
    "OUTPUT_DIR = \"./vectorstore\"\n",
    "MIN_TEXT_THRESHOLD = 100  # If extracted text < this, use OCR\n",
    "OCR_DPI = 200  # Higher = better quality but slower\n",
    "\n",
    "# -------------------------\n",
    "# Text cleaning helper\n",
    "# -------------------------\n",
    "def clean_ocr_text(text):\n",
    "    \"\"\"Remove OCR artifacts, excess whitespace, and common noise\"\"\"\n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    # Remove page markers if they're noise\n",
    "    text = re.sub(r'\\n--- Page \\d+ ---\\n', '\\n', text)\n",
    "    # Remove lines with only special chars/numbers (often headers/footers)\n",
    "    lines = [line for line in text.split('\\n') \n",
    "             if len(line.strip()) > 3 and not re.match(r'^[\\d\\s\\-_|]+$', line.strip())]\n",
    "    text = '\\n'.join(lines)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n ', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------\n",
    "# OCR extraction with cleaning\n",
    "# -------------------------\n",
    "def extract_text_ocr(pdf_path, clean=True):\n",
    "    \"\"\"Extract text via OCR with optional cleaning\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as path:\n",
    "            images = convert_from_path(pdf_path, dpi=OCR_DPI, output_folder=path)\n",
    "            for i, img in enumerate(images):\n",
    "                page_text = pytesseract.image_to_string(img, lang='eng')\n",
    "                text += f\"\\n--- Page {i+1} ---\\n{page_text}\"\n",
    "        \n",
    "        if clean:\n",
    "            text = clean_ocr_text(text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  OCR failed for {os.path.basename(pdf_path)}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# -------------------------\n",
    "# Smart PDF loading with auto-OCR detection\n",
    "# -------------------------\n",
    "def load_pdfs_smart():\n",
    "    \"\"\"Load PDFs with intelligent OCR fallback\"\"\"\n",
    "    docs = []\n",
    "    processing_log = []\n",
    "    \n",
    "    pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    print(f\"üìÇ Found {len(pdf_files)} PDF files\\n\")\n",
    "    \n",
    "    for file in pdf_files:\n",
    "        pdf_path = os.path.join(DATA_DIR, file)\n",
    "        method_used = \"PyPDFLoader\"\n",
    "        text = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Try standard PDF extraction\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "            text = \" \".join([p.page_content for p in pages])\n",
    "            \n",
    "            # Step 2: Check if extraction was successful\n",
    "            text_length = len(text.strip())\n",
    "            \n",
    "            if text_length < MIN_TEXT_THRESHOLD:\n",
    "                # Text too short ‚Üí likely image-based PDF, use OCR\n",
    "                print(f\"üîç {file}: Extracted only {text_length} chars, switching to OCR...\")\n",
    "                text = extract_text_ocr(pdf_path)\n",
    "                method_used = \"OCR (low text)\"\n",
    "            else:\n",
    "                # Check text quality (ratio of alphanumeric to total)\n",
    "                alphanumeric = sum(c.isalnum() or c.isspace() for c in text)\n",
    "                quality_ratio = alphanumeric / len(text) if len(text) > 0 else 0\n",
    "                \n",
    "                if quality_ratio < 0.5:\n",
    "                    # Mostly garbage characters ‚Üí use OCR\n",
    "                    print(f\"üîç {file}: Poor text quality ({quality_ratio:.2%}), switching to OCR...\")\n",
    "                    text = extract_text_ocr(pdf_path)\n",
    "                    method_used = \"OCR (poor quality)\"\n",
    "                else:\n",
    "                    print(f\"‚úÖ {file}: Standard extraction OK ({text_length} chars)\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Step 3: Fallback to OCR on any error\n",
    "            print(f\"‚ö†Ô∏è  {file}: PyPDFLoader failed ({e}), using OCR...\")\n",
    "            text = extract_text_ocr(pdf_path)\n",
    "            method_used = \"OCR (extraction error)\"\n",
    "        \n",
    "        # Store document\n",
    "        if text.strip():\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": file, \"extraction_method\": method_used}\n",
    "            ))\n",
    "            processing_log.append({\n",
    "                \"file\": file,\n",
    "                \"method\": method_used,\n",
    "                \"text_length\": len(text)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"‚ùå {file}: No text extracted (skipping)\")\n",
    "    \n",
    "    return docs, pd.DataFrame(processing_log)\n",
    "\n",
    "# -------------------------\n",
    "# Build Vectorstore\n",
    "# -------------------------\n",
    "def build_vectorstore():\n",
    "    \"\"\"Build FAISS vectorstore with diagnostics\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Loading PDFs\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    docs, log_df = load_pdfs_smart()\n",
    "    \n",
    "    if not docs:\n",
    "        print(\"\\n‚ùå No documents loaded! Check your PDF directory.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(docs)} PDFs\\n\")\n",
    "    \n",
    "    # Show processing summary\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(log_df.to_string(index=False))\n",
    "    print(f\"\\nOCR Usage: {log_df['method'].str.contains('OCR').sum()} / {len(log_df)} PDFs\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Step 2: Chunk documents\n",
    "    # -------------------------\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Chunking Documents\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Step 3: Create embeddings\n",
    "    # -------------------------\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: Creating Vector Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedder)\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    vectorstore.save_local(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Vectorstore saved at {OUTPUT_DIR}\\n\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Step 4: Inspect chunks\n",
    "    # -------------------------\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 4: Chunk Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df_chunks = pd.DataFrame({\n",
    "        \"source_pdf\": [c.metadata[\"source\"] for c in chunks],\n",
    "        \"extraction_method\": [c.metadata.get(\"extraction_method\", \"unknown\") for c in chunks],\n",
    "        \"chunk_text\": [c.page_content for c in chunks],\n",
    "        \"chunk_length\": [len(c.page_content) for c in chunks]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìÑ Sample chunks (first 3):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, row in df_chunks.head(3).iterrows():\n",
    "        print(f\"\\nSource: {row['source_pdf']} ({row['extraction_method']})\")\n",
    "        print(f\"Text: {row['chunk_text'][:200]}...\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    stats = df_chunks.groupby(\"source_pdf\").agg(\n",
    "        total_chunks=(\"chunk_text\", \"count\"),\n",
    "        avg_chunk_length=(\"chunk_length\", \"mean\"),\n",
    "        method=(\"extraction_method\", \"first\")\n",
    "    ).sort_values(by=\"total_chunks\", ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Chunk Statistics by PDF:\")\n",
    "    print(stats.to_string())\n",
    "    \n",
    "    # -------------------------\n",
    "    # Step 5: Test retrieval\n",
    "    # -------------------------\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 5: Testing Retrieval\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_query = \"What information is available in these documents?\"\n",
    "    results = vectorstore.similarity_search(test_query, k=3)\n",
    "    \n",
    "    print(f\"\\nüîé Query: '{test_query}'\")\n",
    "    print(f\"üìå Top 3 results:\\n\")\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"{i}. Source: {r.metadata['source']}\")\n",
    "        print(f\"   Method: {r.metadata.get('extraction_method', 'unknown')}\")\n",
    "        print(f\"   Text: {r.page_content[:250]}...\")\n",
    "        print()\n",
    "    \n",
    "    return vectorstore, chunks, df_chunks\n",
    "\n",
    "# -------------------------\n",
    "# Run the pipeline\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    vectorstore, chunks, df_chunks = build_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f087b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Loading PDFs\n",
      "============================================================\n",
      "üìÇ Found 5 PDF files\n",
      "\n",
      "‚úÖ gsh-13_v1_panic_january24_final.pdf: Standard extraction OK (23841 chars)\n",
      "üñºÔ∏è  How-can-I-help-someone-having-a-panic-attack-Accessible.pdf: Detected image-heavy PDF ‚Üí using OCR directly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  OCR failed for How-can-I-help-someone-having-a-panic-attack-Accessible.pdf: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\lenovo\\\\AppData\\\\Local\\\\Temp\\\\tmphh3kl5hi\\\\bd2897ef-219d-423e-90ba-1b7b6d386cd2-2.ppm'\n",
      "‚ùå How-can-I-help-someone-having-a-panic-attack-Accessible.pdf: No text extracted (skipping)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Cannot set gray stroke color because /'P19' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P19' is an invalid float value\n",
      "Cannot set gray stroke color because /'P24' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P24' is an invalid float value\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf: Standard extraction OK (13366 chars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='./data/docs\\\\Panic-ER-final-2022.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ panic-a-self-help-guide.pdf: Standard extraction OK (34936 chars)\n",
      "‚úÖ Panic-ER-final-2022.pdf: Standard extraction OK (6360 chars)\n",
      "\n",
      "‚úÖ Successfully loaded 4 PDFs\n",
      "\n",
      "============================================================\n",
      "Processing Summary\n",
      "============================================================\n",
      "                                                            file      method  text_length\n",
      "                             gsh-13_v1_panic_january24_final.pdf PyPDFLoader        23842\n",
      "Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf PyPDFLoader        13366\n",
      "                                     panic-a-self-help-guide.pdf PyPDFLoader        34936\n",
      "                                         Panic-ER-final-2022.pdf PyPDFLoader         6360\n",
      "\n",
      "OCR Usage: 0 / 4 PDFs\n",
      "\n",
      "============================================================\n",
      "STEP 2: Chunking Documents\n",
      "============================================================\n",
      "‚úÖ Created 175 chunks\n",
      "\n",
      "============================================================\n",
      "STEP 3: Creating Vector Embeddings\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_20600\\182300563.py:198: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vectorstore saved at ./vectorstore\n",
      "\n",
      "============================================================\n",
      "STEP 4: Chunk Analysis\n",
      "============================================================\n",
      "\n",
      "üìä CHUNKING EFFICIENCY REPORT\n",
      "------------------------------------------------------------\n",
      "üìò Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf: 30 chunks | Avg 454.9 chars | StdDev 63.2 | Efficiency 91.0%\n",
      "  ‚úÖ Balanced chunking.\n",
      "üìò Panic-ER-final-2022.pdf: 15 chunks | Avg 449.9 chars | StdDev 99.6 | Efficiency 90.0%\n",
      "  ‚úÖ Balanced chunking.\n",
      "üìò gsh-13_v1_panic_january24_final.pdf: 53 chunks | Avg 467.6 chars | StdDev 43.1 | Efficiency 93.5%\n",
      "  ‚úÖ Balanced chunking.\n",
      "üìò panic-a-self-help-guide.pdf: 77 chunks | Avg 461.0 chars | StdDev 38.5 | Efficiency 92.2%\n",
      "  ‚úÖ Balanced chunking.\n",
      "\n",
      "============================================================\n",
      "STEP 4.5: Sample Texts from Each PDF\n",
      "============================================================\n",
      "\n",
      "üìò Panic Attacks & Panic Disorder_ Causes, Symptoms & Treatment.pdf ‚Äî showing 5 random chunks:\n",
      "------------------------------------------------------------\n",
      "1. [PyPDFLoader] Medically Reviewed Last reviewed on 02/12/2023. Learn more about the Health Library and our editorial process. References Appointments866.588.2264 APPOINTMENTS & LOCATIONS REQUEST AN APPOINTMENT Actions Appointments & Access Accepted Insurance Events Calendar Financial Assistance Give to Cleveland Clinic Pay Your Bill Online Refer a Patient Phone Directory Virtual Second Opinions Virtual Visits Bl...\n",
      "\n",
      "2. [PyPDFLoader] behaviors. Specific types of psychotherapy that can help with panic attacks and panic disorder include: Cognitive behavioral therapy (CBT): In this type of therapy, you discuss your thoughts and emotions with a mental health professional, such as a licensed counselor or psychologist. This specialist helps identify panic attack triggers so you can change your thinking, behaviors and reactions. As y...\n",
      "\n",
      "3. [PyPDFLoader] your thoughts and ground yourself. It‚Äôs important to seek medical treatment, like medication and psychotherapy, if you‚Äôre having frequent panic attacks. How can I help someone having a panic attack? If someone you know is having a panic attack, you can do the following to help them: Stay with them and remain calm. Ask them what they need. Speak to them in short, simple sentences. Help them focus o...\n",
      "\n",
      "4. [PyPDFLoader] Antidepressants: Certain antidepressant medications can make panic attacks less frequent or less severe. Healthcare providers may prescribe serotonin-selective reuptake inhibitors (SSRIs) or serotonin-norepinephrine reuptake inhibitors (SNRIs). SSRIs include fluoxetine (Prozac¬Æ) and paroxetine (Paxil¬Æ). SNRIs include duloxetine (Cymbalta¬Æ) and venlafaxine (Effexor¬Æ). Anti-anxiety medications: Prov...\n",
      "\n",
      "5. [PyPDFLoader] perceive and handle fear and anxiety. Researchers think that dysfunction of your amygdala ‚Äî the part of your brain that processes fear and other emotions ‚Äî may be at the root of these conditions. They also think chemical imbalances in gamma-aminobutyric acid (GABA), cortisol and serotonin may play a large role. Your risk of having panic disorder increases if you have: A family history: Anxiety dis...\n",
      "\n",
      "\n",
      "üìò Panic-ER-final-2022.pdf ‚Äî showing 5 random chunks:\n",
      "------------------------------------------------------------\n",
      "1. [PyPDFLoader] having panic attacks.  Your family doctor is the best person to  speak to about learning how you can  deal with panic attacks. The doctor will  help you to feel more calm and relaxed.  If you are not sure if you are having  panic attacks you should talk to your  support worker, if you have one.  You could try to:   Breathe more slowly. This might stop you  feeling faint or dizzy.  Stop worrying yo...\n",
      "\n",
      "2. [PyPDFLoader] Support for people who suffer from panic   attacks, OCD and phobias.  Telephone: 0300 772 9844  www.nopanic.org.uk  Rethink  Advice and information for people with  mental health problems.  Advice Service: 0808 801 0525  Email: advice@rethink.org  www.rethink.org  You can also get help and information  from your doctor or nurse. 11 Easy Read version developed by:  ‚Ä¢ Skills for People, Telephone: 0...\n",
      "\n",
      "3. [PyPDFLoader] Panic  Easy read information Table of contents  How can this guide help me? _____________________________ 3  What is panic? _____________________________________________ 4  What is a panic attack?______________________________________ 5  These are some of the things you might feel if you are having a  panic attack: _______________________________________________ 6  Why do people have panic attacks...\n",
      "\n",
      "4. [PyPDFLoader] this leaflet please get in touch.   This information can be made available in a range of formats on  request (eg Braille, audio, larger print, BSL or other languages).  Please contact the Patient Information Centre Tel: 0191 246 7288  Published by the Patient Information Centre  2022 Copyright, Cumbria, Northumberland, Tyne and Wear NHS  Foundation Trust  Ref, PIC/635/0422 April 2022 V3  www.cntw....\n",
      "\n",
      "5. [PyPDFLoader] a beat  Chest pains, your heart seems to stop   then start again suddenly  Breathing too fast or you feel you are  short of breath and can‚Äôt breathe  Head pounding  Fingers, toes or lips feel numb or you  feel them tingling  You feel sick or you think you can‚Äôt  swallow  You think you are going to faint 7  Emotions or feelings  You feel very frightened  Nothing feels real  You feel anxious when yo...\n",
      "\n",
      "\n",
      "üìò gsh-13_v1_panic_january24_final.pdf ‚Äî showing 5 random chunks:\n",
      "------------------------------------------------------------\n",
      "1. [PyPDFLoader] of the heart attack Panic attack vs heart attack Adapted from Andrews G and Jenkins, R, 1998, Managing Mental Disorders, (UK Edition) Sydney World Health Organisation Collaboration Centre for Mental Health and Substance Abuse. 10 Factors that keep panic attacks going There are three main factors that can work  together to keep panic attacks going 1. Selective attention ‚Äì scanning your body for sym...\n",
      "\n",
      "2. [PyPDFLoader] confronting them again and again, you can reduce the anxiety  associated with those situations. Using a graded exposure  hierarchy, we start with situations that are less anxiety provoking  and work up to more difficult situations. This helps us learn that  anxiety levels do come down if we stay in the situation and don‚Äôt  use safety behaviours.  See a graded hierarchy example on page 23. The four...\n",
      "\n",
      "3. [PyPDFLoader] Going to a crowded train station 90 Going to a large supermarket at the busiest time of the day  and do my shopping 80 Going to a small supermarket at the busiest time of day to do  my shopping 75 Going to a large supermarket at a quiet time of day to do my  shopping 65 Going to a small supermarket at a quiet time of the day to do  my shopping 60 Going to a large caf√© in lunch hour and having lunc...\n",
      "\n",
      "4. [PyPDFLoader] oxygen and prepare our body for action.  As we are not actually running away from danger, we do not  use up this extra oxygen.  Because we‚Äôre inhaling more oxygen than our body needs, it  causes an imbalance in our body‚Äôs chemistry.  When we acquire more oxygen than we need, it builds up in  our blood stream over a period of time.  This build-up can happen gradually throughout the day while  we‚Äôre...\n",
      "\n",
      "5. [PyPDFLoader] achieved this, you can then move up to the next exercise on  your hierarchy. ‚Ä¢ Rule 4 ‚Äì Without distraction  It is important when using this technique to be aware of any  safety behaviours that you are doing, or anything that could  distract you from the anxiety. It‚Äôs important to truly feel the  anxiety in order to learn that your anxiety levels will reduce. If  it feels too difficult to drop saf...\n",
      "\n",
      "\n",
      "üìò panic-a-self-help-guide.pdf ‚Äî showing 5 random chunks:\n",
      "------------------------------------------------------------\n",
      "1. [PyPDFLoader] PANIC ATTACKS ARE NOT DANGEROUS. Fife Clinical Psychology Department 3  Lots of people have panic attacks, although they can affect people  in different ways.  Some people have only one; others may have  them for many years.  Some people have them every day, some  people only once in a while.  If you were to ask all of your friends if  they had ever had a panic attack, it is very likely that at le...\n",
      "\n",
      "2. [PyPDFLoader] described earlier are all part of the vicious circle.                                                                      'THREAT'  Sits down ‚Äì \"If I hadn't  sat down I would have  had a heart attack\"  Thinks, \"I'm sure my  heart missed a beat\"  Alarm bell  Physical symptoms of  anxiety, for example  heart thumping  Thinks \"Oh no,  something is wrong\"  Physical symptoms  get worse  Thinks, \"Now I...\n",
      "\n",
      "3. [PyPDFLoader] ...............................................................................................................    ...............................................................................................................    ............................................................................................................... Fife Clinical Psychology Department 5  Your mind (please t...\n",
      "\n",
      "4. [PyPDFLoader] NHS Fife Department of Psychology              Panic         A Self Help Guide                            Help Yourself @ moodcafe.co.uk Index        Introduction   .............................................................. 3    1. Recognising panic   How do I know if I am having a panic attack? ...... 5    2. Understanding panic   What causes it and what keeps it going? ............. 8    3. ...\n",
      "\n",
      "5. [PyPDFLoader] of tea (although that is just as important!).  It is a skill, to be learnt  and practised.  There are relaxation tapes, and sometimes classes,  which can help. Yoga classes can also be helpful.  Your therapist  will be able to give you a relaxation tape, so please ask.   Relaxation tapes teach you to go through the main muscle groups  in your body, tensing and relaxing your muscles.  The tape will...\n",
      "\n",
      "\n",
      "============================================================\n",
      "STEP 5: Testing Retrieval\n",
      "============================================================\n",
      "\n",
      "üîé Query: 'What information is available in these documents?'\n",
      "üìå Top 3 results:\n",
      "\n",
      "1. Source: Panic-ER-final-2022.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: Review date 2025 \n",
      "ISBN:978-1-909664-34-0\n",
      " \n",
      "Follow us on Twitter @cntwnhs and Facebook CNTWNHS...\n",
      "\n",
      "2. Source: gsh-13_v1_panic_january24_final.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: ÔÅ± Facing your fears‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....19\n",
      "ÔÅ± Graded exposure‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.21\n",
      "ÔÅ± My graded hierarchy‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶...24\n",
      "ÔÅ± My exposure diary‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶..........25\n",
      "ÔÅ± Tips for dealing with panic‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.26\n",
      "ÔÅ± Further resources‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶....\n",
      "\n",
      "3. Source: Panic-ER-final-2022.pdf\n",
      "   Method: PyPDFLoader\n",
      "   Text: this leaflet please get in touch.  \n",
      "This information can be made available in a range of formats on \n",
      "request (eg Braille, audio, larger print, BSL or other languages). \n",
      "Please contact the Patient Information Centre Tel: 0191 246 7288 \n",
      "Published by th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Smart RAG with Hybrid OCR Detection + Chunk Efficiency Check + Sample Display\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "DATA_DIR = \"./data/docs\"\n",
    "OUTPUT_DIR = \"./vectorstore\"\n",
    "MIN_TEXT_THRESHOLD = 100\n",
    "OCR_DPI = 200\n",
    "TARGET_CHUNK_SIZE = 500  # for efficiency comparison\n",
    "\n",
    "# -------------------------\n",
    "# Text cleaning helper\n",
    "# -------------------------\n",
    "def clean_ocr_text(text):\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'\\n--- Page \\d+ ---\\n', '\\n', text)\n",
    "    lines = [\n",
    "        line for line in text.split('\\n')\n",
    "        if len(line.strip()) > 3 and not re.match(r'^[\\d\\s\\-_|]+$', line.strip())\n",
    "    ]\n",
    "    text = '\\n'.join(lines)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n ', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------------\n",
    "# OCR extraction with cleaning\n",
    "# -------------------------\n",
    "def extract_text_ocr(pdf_path, clean=True):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as path:\n",
    "            images = convert_from_path(pdf_path, dpi=OCR_DPI, output_folder=path)\n",
    "            for i, img in enumerate(images):\n",
    "                page_text = pytesseract.image_to_string(img, lang='eng')\n",
    "                text += f\"\\n--- Page {i+1} ---\\n{page_text}\"\n",
    "        if clean:\n",
    "            text = clean_ocr_text(text)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  OCR failed for {os.path.basename(pdf_path)}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Check if a PDF has a text layer\n",
    "# -------------------------\n",
    "def has_text_layer(pdf_path):\n",
    "    try:\n",
    "        text = pdfminer_extract_text(pdf_path)\n",
    "        return len(text.strip()) > 200\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Detect image-heavy PDFs\n",
    "# -------------------------\n",
    "def is_image_based(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        image_pages = 0\n",
    "        for page in reader.pages:\n",
    "            if \"/XObject\" in page.get(\"/Resources\", {}):\n",
    "                xObject = page[\"/Resources\"][\"/XObject\"].get_object()\n",
    "                for obj in xObject:\n",
    "                    if xObject[obj].get(\"/Subtype\") == \"/Image\":\n",
    "                        image_pages += 1\n",
    "        return image_pages / max(len(reader.pages), 1) > 0.7\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Detect low-quality text\n",
    "# -------------------------\n",
    "def is_low_quality_text(text):\n",
    "    if len(text.strip()) < 100:\n",
    "        return True\n",
    "    alpha_ratio = sum(c.isalpha() for c in text) / max(len(text), 1)\n",
    "    if alpha_ratio < 0.2:\n",
    "        return True\n",
    "    words = text.split()\n",
    "    avg_word_len = sum(len(w) for w in words) / max(len(words), 1)\n",
    "    if avg_word_len < 3:\n",
    "        return True\n",
    "    non_ascii_ratio = sum(ord(c) > 127 for c in text) / len(text)\n",
    "    if non_ascii_ratio > 0.2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# -------------------------\n",
    "# Smart PDF loading with auto-OCR detection\n",
    "# -------------------------\n",
    "def load_pdfs_smart():\n",
    "    docs = []\n",
    "    processing_log = []\n",
    "    pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".pdf\")]\n",
    "\n",
    "    print(f\"üìÇ Found {len(pdf_files)} PDF files\\n\")\n",
    "\n",
    "    for file in pdf_files:\n",
    "        pdf_path = os.path.join(DATA_DIR, file)\n",
    "        method_used = \"PyPDFLoader\"\n",
    "        text = \"\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Quick pre-check before extraction\n",
    "            if is_image_based(pdf_path):\n",
    "                print(f\"üñºÔ∏è  {file}: Detected image-heavy PDF ‚Üí using OCR directly.\")\n",
    "                text = extract_text_ocr(pdf_path)\n",
    "                method_used = \"OCR (image-based)\"\n",
    "            elif not has_text_layer(pdf_path):\n",
    "                print(f\"üîç {file}: No text layer detected ‚Üí using OCR.\")\n",
    "                text = extract_text_ocr(pdf_path)\n",
    "                method_used = \"OCR (no text layer)\"\n",
    "            else:\n",
    "                # Step 2: Try standard text extraction\n",
    "                loader = PyPDFLoader(pdf_path)\n",
    "                pages = loader.load()\n",
    "                text = \" \".join([p.page_content for p in pages])\n",
    "                text_length = len(text.strip())\n",
    "                if text_length < MIN_TEXT_THRESHOLD or is_low_quality_text(text):\n",
    "                    print(f\"üîç {file}: Weak or low-quality text ‚Üí using OCR.\")\n",
    "                    text = extract_text_ocr(pdf_path)\n",
    "                    method_used = \"OCR (low quality)\"\n",
    "                else:\n",
    "                    print(f\"‚úÖ {file}: Standard extraction OK ({text_length} chars)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  {file}: Extraction error ({e}) ‚Üí using OCR.\")\n",
    "            text = extract_text_ocr(pdf_path)\n",
    "            method_used = \"OCR (error fallback)\"\n",
    "\n",
    "        if text.strip():\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": file, \"extraction_method\": method_used}\n",
    "            ))\n",
    "            processing_log.append({\n",
    "                \"file\": file,\n",
    "                \"method\": method_used,\n",
    "                \"text_length\": len(text)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"‚ùå {file}: No text extracted (skipping)\")\n",
    "\n",
    "    return docs, pd.DataFrame(processing_log)\n",
    "\n",
    "# -------------------------\n",
    "# Build Vectorstore\n",
    "# -------------------------\n",
    "def build_vectorstore():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: Loading PDFs\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    docs, log_df = load_pdfs_smart()\n",
    "    if not docs:\n",
    "        print(\"\\n‚ùå No documents loaded! Check your PDF directory.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(docs)} PDFs\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Processing Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(log_df.to_string(index=False))\n",
    "    print(f\"\\nOCR Usage: {log_df['method'].str.contains('OCR').sum()} / {len(log_df)} PDFs\")\n",
    "\n",
    "    # Step 2: Chunking\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 2: Chunking Documents\")\n",
    "    print(\"=\" * 60)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=TARGET_CHUNK_SIZE,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\\n\")\n",
    "\n",
    "    # Step 3: Embeddings\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 3: Creating Vector Embeddings\")\n",
    "    print(\"=\" * 60)\n",
    "    embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedder)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    vectorstore.save_local(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Vectorstore saved at {OUTPUT_DIR}\\n\")\n",
    "\n",
    "    # Step 4: Chunk Analysis\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 4: Chunk Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    df_chunks = pd.DataFrame({\n",
    "        \"source_pdf\": [c.metadata[\"source\"] for c in chunks],\n",
    "        \"extraction_method\": [c.metadata.get(\"extraction_method\", \"unknown\") for c in chunks],\n",
    "        \"chunk_text\": [c.page_content for c in chunks],\n",
    "        \"chunk_length\": [len(c.page_content) for c in chunks]\n",
    "    })\n",
    "\n",
    "    # === Chunking Quality Check ===\n",
    "    print(\"\\nüìä CHUNKING EFFICIENCY REPORT\")\n",
    "    print(\"-\" * 60)\n",
    "    for pdf, group in df_chunks.groupby(\"source_pdf\"):\n",
    "        avg_len = group[\"chunk_length\"].mean()\n",
    "        std_len = group[\"chunk_length\"].std()\n",
    "        num_chunks = len(group)\n",
    "        efficiency = (avg_len / TARGET_CHUNK_SIZE) * 100\n",
    "        print(f\"üìò {pdf}: {num_chunks} chunks | Avg {avg_len:.1f} chars | StdDev {std_len:.1f} | Efficiency {efficiency:.1f}%\")\n",
    "        if efficiency < 60:\n",
    "            print(\"  ‚ö†Ô∏è  Too small chunks ‚Üí consider increasing chunk_size.\")\n",
    "        elif efficiency > 120:\n",
    "            print(\"  ‚ö†Ô∏è  Chunks too large ‚Üí may affect retrieval speed.\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Balanced chunking.\")\n",
    "\n",
    "    # Step 4.5: Show Random Samples per PDF\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 4.5: Sample Texts from Each PDF\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pdf, group in df_chunks.groupby(\"source_pdf\"):\n",
    "        print(f\"\\nüìò {pdf} ‚Äî showing 5 random chunks:\\n\" + \"-\" * 60)\n",
    "        samples = group.sample(min(5, len(group)), random_state=42)\n",
    "        for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "            preview = row[\"chunk_text\"][:400].replace(\"\\n\", \" \")\n",
    "            print(f\"{i}. [{row['extraction_method']}] {preview}...\\n\")\n",
    "\n",
    "    # Step 5: Test retrieval\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STEP 5: Testing Retrieval\")\n",
    "    print(\"=\" * 60)\n",
    "    test_query = \"What information is available in these documents?\"\n",
    "    results = vectorstore.similarity_search(test_query, k=3)\n",
    "    print(f\"\\nüîé Query: '{test_query}'\\nüìå Top 3 results:\\n\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"{i}. Source: {r.metadata['source']}\")\n",
    "        print(f\"   Method: {r.metadata.get('extraction_method', 'unknown')}\")\n",
    "        print(f\"   Text: {r.page_content[:250]}...\\n\")\n",
    "\n",
    "    return vectorstore, chunks, df_chunks\n",
    "\n",
    "# -------------------------\n",
    "# Run the pipeline\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    vectorstore, chunks, df_chunks = build_vectorstore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60775ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Question: What are some grounding techniques to stop a panic attack?\n",
      "\n",
      "üí¨ Answer:\n",
      " {\n",
      "  \"response\": \"While it's difficult to immediately stop a panic attack once it begins, there are techniques to help you feel safer and manage the experience. Some suggestions include: \\n\\n*   **Avoid caffeine, alcohol, and smoking:** These substances can worsen panic attacks.\\n*   **Make lifestyle changes:** Incorporate regular exercise to manage stress, relieve tension, and boost your mood. Eating regular, healthy meals and avoiding processed foods and drinks can also help stabilize blood sugar levels.\\n*   **Challenge your thoughts:** Avoid scanning your body for evidence of something being wrong. Instead, focus on something to make yourself feel safe, such as sitting down if you feel faint or lying down if you think you're having a heart attack.\\n\\nIf panic attacks are a recurring issue, consider exploring resources like the Centre for Clinical Interventions (CCI) workbooks for additional support: https://www.cci.health.wa.gov.au/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Gemini RAG Integration in Jupyter ---\n",
    "\n",
    "# Install dependencies (run once per environment)\n",
    "#ip install --quiet google-genai langchain_community sentence-transformers faiss-cpu\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "from google import genai\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# --- SETUP ---\n",
    "GOOGLE_API_KEY = \"AIzaSyBF0VcET3kuCYmrdixWz3oz1nHzoC8wfoI\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Load your vectorstore (adjust path if needed)\n",
    "VECTORSTORE_PATH = \"./vectorstore\"\n",
    "embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(VECTORSTORE_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# --- PROMPT TEMPLATE ---\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an empathetic assistant helping users manage anxiety and panic.\\n\"\n",
    "    \"Provide calm, practical, and kind guidance based on the provided context.\\n\"\n",
    "    \"If a user mentions crisis or self-harm, encourage professional help or helplines.\"\n",
    ")\n",
    "\n",
    "def build_prompt(question, context):\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{context}\\n\\nUser question:\\n{question}\\n\\nAnswer:\"\n",
    "\n",
    "# --- GEMINI CALL FUNCTION ---\n",
    "from google.genai import types\n",
    "\n",
    "def call_gemini(prompt):\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=SYSTEM_PROMPT,\n",
    "        max_output_tokens=400,\n",
    "        temperature=0.2,\n",
    "        response_mime_type=\"application/json\"\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,\n",
    "        config=config\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "\n",
    "# --- MAIN QA FUNCTION ---\n",
    "def ask(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    prompt = build_prompt(question, context)\n",
    "    answer = call_gemini(prompt)\n",
    "    return answer\n",
    "\n",
    "# --- TEST RUN ---\n",
    "question = \"What are some grounding techniques to stop a panic attack?\"\n",
    "print(\"üß© Question:\", question)\n",
    "print(\"\\nüí¨ Answer:\\n\", ask(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2478dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- SETUP ---\n",
    "GOOGLE_API_KEY = \"AIzaSyBF0VcET3kuCYmrdixWz3oz1nHzoC8wfoI\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75585f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Question: What are some grounding techniques to stop a panic attack?\n",
      "\n",
      "üí¨ Answer:\n",
      " It sounds like you're looking for ways to manage panic attacks, and grounding techniques can be really helpful. It's great that you're being proactive about this.\n",
      "\n",
      "Here are a few ideas based on the information you shared, keeping in mind that everyone is different, and it might take some experimenting to find what works best for you:\n",
      "\n",
      "*   **Focus on your senses:** The goal is to bring you back to the present moment. You can try the 5-4-3-2-1 method. Name 5 things you can see, 4 things you can touch, 3 things you can hear, 2 things you can smell, and 1 thing you can taste.\n",
      "\n",
      "*   **Safe actions:** Is there something you can do to make yourself feel safer in the moment? The information mentions that some people try gulping air if they feel like they are suffocating, sitting down if they feel faint, or lying down if they think they are having a heart attack.\n",
      "\n",
      "It's important to remember that panic attacks can feel very intense, and it's okay to seek help. The information mentions that many people go to the emergency room during their first panic attack, thinking something dangerous is happening. If you ever feel like you need immediate medical attention, please don't hesitate to seek it.\n",
      "\n",
      "Also, the Centre for Clinical Interventions (CCI) website ([https://www.cci.health.wa.gov.au/](https://www.cci.health.wa.gov.au/)) offers workbooks that might be helpful.\n",
      "\n",
      "In addition to these techniques, the information also suggests some lifestyle changes that can help reduce the frequency and intensity of panic attacks:\n",
      "\n",
      "*   **Avoid stimulants:** Try to limit or avoid caffeine, alcohol, and cigarettes.\n",
      "*   **Exercise regularly:** Physical activity can help manage stress and improve your mood.\n",
      "*   **Eat a healthy diet:** Regular meals and avoiding\n"
     ]
    }
   ],
   "source": [
    "# --- Gemini RAG Integration in Jupyter ---\n",
    "\n",
    "# Install dependencies (run once per environment)\n",
    "# !pip install --quiet google-genai langchain_community sentence-transformers faiss-cpu\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "from google import genai\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from google.genai import types\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Load your vectorstore (adjust path if needed)\n",
    "VECTORSTORE_PATH = \"./vectorstore\"  # path where your FAISS vectorstore is saved\n",
    "embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(VECTORSTORE_PATH, embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# --- PROMPT TEMPLATE ---\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an empathetic assistant helping users manage anxiety and panic.\\n\"\n",
    "    \"Provide calm, practical, and kind guidance based on the provided context.\\n\"\n",
    "    \"If a user mentions crisis or self-harm, encourage professional help or helplines.\"\n",
    ")\n",
    "\n",
    "# --- GEMINI CALL FUNCTION (flash model, system prompt merged) ---\n",
    "def call_gemini_with_context(context, question):\n",
    "    # Merge system prompt, context, and user question into a single string\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\nContext:\\n{context}\\n\\nUser question:\\n{question}\\n\\nAnswer:\"\n",
    "\n",
    "    # Generate response using flash model\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[full_prompt],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.2,\n",
    "            max_output_tokens=400\n",
    "        )\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "# --- MAIN QA FUNCTION ---\n",
    "def ask(question):\n",
    "    # Step 1: Retrieve relevant PDF chunks\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Step 2: Call Gemini with context + question\n",
    "    answer = call_gemini_with_context(context, question)\n",
    "    return answer\n",
    "\n",
    "# --- TEST RUN ---\n",
    "question = \"What are some grounding techniques to stop a panic attack?\"\n",
    "print(\"üß© Question:\", question)\n",
    "print(\"\\nüí¨ Answer:\\n\", ask(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c3a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
